{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext('local', 'Rivet ETL Challenge') \n",
    "spark = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "paymentsDF = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/Users/manikhossain/Downloads/Rivet-Coding-Challenge/payments.csv\") \\\n",
    "                       .withColumn(\"payDate\", to_date(col(\"payment_date\"),\"yyyy-MM-dd\")) \\\n",
    "                       .drop(\"payment_date\").withColumnRenamed(\"payDate\", \"payment_date\")                      \n",
    "\n",
    "invoicesDF = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/Users/manikhossain/Downloads/Rivet-Coding-Challenge/invoices.csv\") \\\n",
    "                      .withColumn(\"invDate\", to_date(col(\"invoice_date\"),\"yyyy-MM-dd\")) \\\n",
    "                      .drop(\"invoice_date\").withColumnRenamed(\"invDate\", \"invoice_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+\n",
      "|customer_id|pct_90|pct_70|\n",
      "+-----------+------+------+\n",
      "|          3|2450.0|2350.0|\n",
      "|          5| 725.0| 675.0|\n",
      "|          1|1000.0|1000.0|\n",
      "|          4|3000.0|3000.0|\n",
      "|          2|4650.0|3950.0|\n",
      "+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "percentileDF = invoicesDF.groupby('customer_id').agg(\n",
    "                             expr('percentile(invoice_amount, array(0.90))')[0].alias('pct_90'),\n",
    "                             expr('percentile(invoice_amount, array(0.70))')[0].alias('pct_70'))\n",
    "percentileDF.createOrReplaceTempView(\"percentileTbl\")\n",
    "percentileDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #### processed-customers.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "paymentsDF.createOrReplaceTempView(\"payments\")\n",
    "invoicesDF.createOrReplaceTempView(\"invoices\")\n",
    "\n",
    "spark.sql(\"SELECT i.customer_id, i.customer_name, sum(i.invoice_amount) total_invoiced_amount, \\\n",
    "          count(*) total_invoice_count, sum(i.invoice_balance) unpaid_amount, \\\n",
    "          count( (CASE WHEN i.invoice_balance > 0.0 then 1 ELSE null END) ) unpaid_count,\\\n",
    "          min(i.invoice_date) first_invoice_date\\\n",
    "          FROM invoices i group by i.customer_id, i.customer_name\").createOrReplaceTempView(\"aggregatedCustomers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------------------+-------------------+-------------+------------+------------------+\n",
      "|customer_id|customer_name|total_invoiced_amount|total_invoice_count|unpaid_amount|unpaid_count|first_invoice_date|\n",
      "+-----------+-------------+---------------------+-------------------+-------------+------------+------------------+\n",
      "|          4|      Netflix|               6000.0|                  2|        500.0|           1|        2021-05-01|\n",
      "|          5|         Meta|               1250.0|                  2|        750.0|           1|        2021-06-15|\n",
      "|          3|       Google|               4500.0|                  2|       2500.0|           1|        2021-03-01|\n",
      "|          2|        Apple|               6500.0|                  2|       2000.0|           1|        2021-01-15|\n",
      "|          1|    Microsoft|               3000.0|                  3|       1000.0|           1|        2021-01-01|\n",
      "+-----------+-------------+---------------------+-------------------+-------------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from aggregatedCustomers\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select b.*, i.invoice_amount first_invoice_amount from aggregatedCustomers b \\\n",
    "          left join invoices i \\\n",
    "          on b.first_invoice_date =  i.invoice_date and b.customer_id = i.customer_id\").createOrReplaceTempView(\"aggregatedTbl\")\n",
    "\n",
    "spark.sql(\"with base as (select customer_id, max(payment_date) payment_date from payments group by customer_id)\\\n",
    "          select b.*, p.payment_amount from base b \\\n",
    "          inner join payments p \\\n",
    "          on b.payment_date =  p.payment_date and  b.customer_id = p.customer_id\").createOrReplaceTempView(\"latestPaymentInfo\")\n",
    "\n",
    "\n",
    "processedCustomerDF = spark.sql(\"select pc.*, p.payment_date last_payment_date, p.payment_amount last_payment_amount,\\\n",
    "                        CASE WHEN pc.total_invoiced_amount >= pt.pct_90 THEN 'High' \\\n",
    "                             WHEN pc.total_invoiced_amount > pt.pct_70 and pc.total_invoiced_amount < pt.pct_90 THEN 'Medium' \\\n",
    "                             ELSE 'Low' \\\n",
    "                        END AS customer_segment\\\n",
    "                        from aggregatedTbl pc \\\n",
    "                        left join latestPaymentInfo p on pc.customer_id = p.customer_id\\\n",
    "                        left join percentileTbl pt on pc.customer_id = pt.customer_id\")\n",
    "\n",
    "processedCustomerDF.createOrReplaceTempView(\"processedCustomersTbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #### processed-invoices.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------------------+-------------------+-------------+------------+------------------+--------------------+-----------------+-------------------+----------------+\n",
      "|customer_id|customer_name|total_invoiced_amount|total_invoice_count|unpaid_amount|unpaid_count|first_invoice_date|first_invoice_amount|last_payment_date|last_payment_amount|customer_segment|\n",
      "+-----------+-------------+---------------------+-------------------+-------------+------------+------------------+--------------------+-----------------+-------------------+----------------+\n",
      "|          4|      Netflix|               6000.0|                  2|        500.0|           1|        2021-05-01|                3000|       2021-07-31|               2500|            High|\n",
      "|          5|         Meta|               1250.0|                  2|        750.0|           1|        2021-06-15|                 500|       2021-07-15|                500|            High|\n",
      "|          3|       Google|               4500.0|                  2|       2500.0|           1|        2021-03-01|                2000|       2021-04-01|               1000|            High|\n",
      "|          2|        Apple|               6500.0|                  2|       2000.0|           1|        2021-01-15|                1500|       2021-08-15|               3000|            High|\n",
      "|          1|    Microsoft|               3000.0|                  3|       1000.0|           1|        2021-01-01|                1000|       2021-06-01|               1000|            High|\n",
      "+-----------+-------------+---------------------+-------------------+-------------+------------+------------------+--------------------+-----------------+-------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from processedCustomersTbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select *, \\\n",
    "          CASE WHEN payment_terms = 'Net 30' THEN date_add(invoice_date, 30) \\\n",
    "               WHEN payment_terms = 'Net 45' THEN date_add(invoice_date, 45) \\\n",
    "               WHEN payment_terms = 'Net 60' THEN date_add(invoice_date, 60) \\\n",
    "               ELSE invoice_date \\\n",
    "          END AS invoice_due_date \\\n",
    "          from invoices\").createOrReplaceTempView(\"populatedInvoice\")\n",
    "\n",
    "\n",
    "processedInvoiceDF = spark.sql(\"select i.customer_id, i.customer_name, i.invoice_id, i.invoice_amount, i.invoice_balance,\\\n",
    "          i.invoice_date, i.payment_terms, i.invoice_due_date,\\\n",
    "          CASE WHEN current_date() > i.invoice_due_date  THEN true ELSE false \\\n",
    "          END AS invoice_overdue, pc.customer_segment\\\n",
    "          from populatedInvoice i left join processedCustomersTbl pc on i.customer_id = pc.customer_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #### processed-payments.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedPaymentDF = spark.sql(\"select p.customer_id, pc.customer_name, p.payment_id, p.payment_amount, p.payment_date, p.invoice_id, pc.customer_segment \\\n",
    "          from payments p left join processedCustomersTbl pc on p.customer_id = pc.customer_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedPaymentDF.toPandas().to_csv('processed-payment.csv', index=False)\n",
    "processedCustomerDF.toPandas().to_csv('processed-customer.csv', index=False)\n",
    "processedInvoiceDF.toPandas().to_csv('processed-invoice.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
